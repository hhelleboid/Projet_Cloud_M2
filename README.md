# Projet_Cloud_M2

# ğŸ¤– Assistant Documentaire RAG (Local)

Ce projet est un assistant intelligent (RAG - Retrieval Augmented Generation) capable de rÃ©pondre Ã  des questions basÃ©es sur vos propres documents PDF.

Il fonctionne **entiÃ¨rement en local** en utilisant **Ollama** pour le LLM et les embeddings et **ChromaDB** pour la base de donnÃ©es vectorielle.

## âœ¨ FonctionnalitÃ©s

- **Upload de PDF** : Ajoutez vos documents directement via l'interface.
- **Indexation automatique** : DÃ©coupage intelligent (Chunking) et vectorisation des textes.
- **Recherche Hybride** : Utilise la recherche vectorielle + un Reranking pour une prÃ©cision maximale.
- **Historique de chat** : Sauvegarde automatique de la conversation.
- **100% Local** : Aucune donnÃ©e n'est envoyÃ©e dans le cloud (nÃ©cessite un bon CPU).

---

## ğŸ› ï¸ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© :

1. **Python 3.10+** : [TÃ©lÃ©charger Python](https://www.python.org/downloads/)
2. **Ollama** : [TÃ©lÃ©charger Ollama](https://ollama.com/)
3. **Git** (pour cloner le projet).

---

## ğŸš€ Installation

### 1. Cloner ou tÃ©lÃ©charger le projet

```bash
git clone <votre-repo-url>
cd Projet_Cloud_M2/app
```

### 2. CrÃ©er l'environnement virtuel

Sous window : 

```bash
python -m venv venv .\venv\Scripts\activate
```

Sous Mac/Linux :

```bash
python3 -m venv venv source venv/bin/activate
```

### 3. Installer les dÃ©pendances

Pour installer les dÃ©pendances effectuer la commande suivante :

```bash
pip install -r requirements.txt
```

## ğŸ¦™ Configuration des ModÃ¨les Ollama

Ce projet nÃ©cessite deux types de modÃ¨les pour fonctionner. Vous devez les tÃ©lÃ©charger via votre terminal (CMD ou PowerShell) une fois Ollama installÃ©.

### 1. ModÃ¨le d'Embedding 

Sert Ã  transformer le texte en vecteurs mathÃ©matiques.

```bash
ollama pull nomic-embed-text
```

### 2. ModÃ¨le de Langage (LLM)

```bash 
ollama pull gemma3:1b
```

Note : Assurez-vous que l'application Ollama tourne en arriÃ¨re-plan (icÃ´ne dans la barre des tÃ¢ches) ou lancez ``ollama serve`` dans un terminal sÃ©parÃ©.


## âš™ï¸ Configuration (.env)
Le projet utilise des variables d'environnement qui seront stockÃ©s comme secret dans le Azure Key Vault.

## â–¶ï¸ DÃ©marrage de l'application

Une fois tout installÃ©, lancez l'interface Streamlit :

```bash 
streamlit run app/query.py
```

## ğŸ³ DÃ©marrage rapide avec Docker 

Au lieu d'installer Python et Ollama manuellement, vous pouvez lancer tout le projet (Frontend + Backend) en une seule commande grÃ¢ce Ã  **Docker Compose**.

```bash 
docker-compose up --build
```

## ğŸ“‚ Structure du projet

```text
Projet_Cloud_M2/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/         # Configuration du pipeline CI GitHub Actions
â”‚          â”œâ”€â”€ ci-test.yml #  Exec Test CI
â”œâ”€â”€ app/                   # FRONTEND (Streamlit)
â”‚   â”œâ”€â”€ Dockerfile         # Configuration de l'image Frontend
â”‚   â”œâ”€â”€ query.py           # Interface principale & Logique RAG
â”‚   â”œâ”€â”€ chunking.py        # Script de dÃ©coupage et d'ingestion des PDF
â”‚   â”œâ”€â”€ requirements.txt   # DÃ©pendances Python
â”‚   â””â”€â”€ data_pdf/          # Dossier de stockage temporaire des PDF
â”œâ”€â”€ backend_ollama/        # BACKEND (Ollama)
â”‚   â”œâ”€â”€ Dockerfile         # Configuration de l'image Backend
â”‚   â””â”€â”€ entrypoint.sh      # Script d'installation des modÃ¨les dans l'image
â”œâ”€â”€ tests/                 # TESTS UNITAIRES
    â””â”€â”€ test_rag_pipeline.py # Tests de configuration
```


